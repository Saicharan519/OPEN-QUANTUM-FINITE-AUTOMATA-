{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ce597",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PROMOTER_CSV_PATH = \"/content/drive/MyDrive/Colab Notebooks/promoter_dataset.csv\"\n",
    "except ImportError:\n",
    "    PROMOTER_CSV_PATH = \"promoter_dataset.csv\"\n",
    "\n",
    "print(\"Using promoter CSV at:\", PROMOTER_CSV_PATH)\n",
    "\n",
    "# ----------------- Imports -----------------\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "NUC2IDX = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3}\n",
    "\n",
    "\n",
    "def clean_seq(seq: str) -> str:\n",
    "    \"\"\"Uppercase and replace non-ACGT characters with 'A'.\"\"\"\n",
    "    seq = seq.upper().strip()\n",
    "    return \"\".join([ch if ch in NUC2IDX else \"A\" for ch in seq])\n",
    "\n",
    "\n",
    "def load_sequence_csv(path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"Load CSV with columns: sequence,label.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"CSV not found at: {path}\")\n",
    "\n",
    "    data = []\n",
    "    with open(path, \"r\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            seq = clean_seq(row[\"sequence\"])\n",
    "            label = int(row[\"label\"])\n",
    "            data.append((seq, label))\n",
    "    return data\n",
    "\n",
    "\n",
    "def split_dataset(data: List[Tuple[str, int]], val_ratio: float = 0.2):\n",
    "    random.shuffle(data)\n",
    "    n_val = int(len(data) * val_ratio)\n",
    "    return data[n_val:], data[:n_val]\n",
    "\n",
    "\n",
    "def batchify(data: List[Tuple[str, int]], batch_size: int):\n",
    "    random.shuffle(data)\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        seqs = [x[0] for x in batch]\n",
    "        labels = torch.tensor([x[1] for x in batch], dtype=torch.long)\n",
    "        yield seqs, labels\n",
    "\n",
    "\n",
    "# ----------------- Improved OQFA model -----------------\n",
    "class ImprovedOQFA(nn.Module):\n",
    "    \"\"\"\n",
    "    Stable quantum-inspired classifier:\n",
    "    - orthogonal transitions\n",
    "    - normalized hidden states\n",
    "    - dropout before measurement\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_states: int = 32, num_classes: int = 2, dropout_p: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.num_states = num_states\n",
    "\n",
    "        self.transitions = nn.Parameter(torch.empty(4, num_states, num_states))\n",
    "        for i in range(4):\n",
    "            nn.init.orthogonal_(self.transitions.data[i])\n",
    "\n",
    "        self.initial_state = nn.Parameter(torch.zeros(num_states))\n",
    "        with torch.no_grad():\n",
    "            self.initial_state[0] = 1.0\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.classifier = nn.Linear(num_states, num_classes)\n",
    "\n",
    "    def step(self, h: torch.Tensor, ch: str) -> torch.Tensor:\n",
    "        idx = NUC2IDX.get(ch, 0)\n",
    "        h = torch.matmul(h, self.transitions[idx])\n",
    "        return h / (h.norm() + 1e-8)\n",
    "\n",
    "    def forward_on_sequence(self, seq: str) -> torch.Tensor:\n",
    "        h = self.initial_state\n",
    "        for ch in seq:\n",
    "            h = self.step(h, ch)\n",
    "        h = self.dropout(h)\n",
    "        return self.classifier(h)\n",
    "\n",
    "    def forward(self, seqs: List[str]) -> torch.Tensor:\n",
    "        return torch.stack([self.forward_on_sequence(s) for s in seqs], dim=0)\n",
    "\n",
    "\n",
    "# ----------------- Training & evaluation -----------------\n",
    "def evaluate_model(model: nn.Module, data: List[Tuple[str, int]], batch_size: int = 16, silent=False):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seqs, labels in batchify(data, batch_size):\n",
    "            labels = labels.to(device)\n",
    "            preds = model(seqs).argmax(dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    acc = total_correct / max(1, total_samples)\n",
    "    if not silent:\n",
    "        print(f\"Accuracy: {acc:.3f}\")\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train_oqfa(\n",
    "    train_data: List[Tuple[str, int]],\n",
    "    val_data: List[Tuple[str, int]],\n",
    "    num_states: int = 32,\n",
    "    num_epochs: int = 80,\n",
    "    batch_size: int = 16,\n",
    "    lr: float = 3e-3,\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    model = ImprovedOQFA(num_states=num_states, num_classes=2, dropout_p=0.3).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for seqs, labels in batchify(train_data, batch_size):\n",
    "            labels = labels.to(device)\n",
    "            logits = model(seqs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        train_loss = total_loss / total_samples\n",
    "        train_acc = total_correct / total_samples\n",
    "        val_acc = evaluate_model(model, val_data, batch_size, silent=True)\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(\n",
    "                f\"Epoch {epoch:02d} | \"\n",
    "                f\"Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Train Acc: {train_acc:.3f} | \"\n",
    "                f\"Val Acc: {val_acc:.3f}\"\n",
    "            )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ----------------- Main execution -----------------\n",
    "print(\"\\nLoading PROMOTER dataset from:\", PROMOTER_CSV_PATH)\n",
    "promoter_data = load_sequence_csv(PROMOTER_CSV_PATH)\n",
    "print(\"Total promoter samples:\", len(promoter_data))\n",
    "print(\"First 3 samples:\", promoter_data[:3])\n",
    "\n",
    "train_data, val_data = split_dataset(promoter_data, val_ratio=0.2)\n",
    "print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}\")\n",
    "\n",
    "model = train_oqfa(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    num_states=32,\n",
    "    num_epochs=80,\n",
    "    batch_size=16,\n",
    "    lr=3e-3,\n",
    ")\n",
    "\n",
    "print(\"\\nFinal evaluation on validation set:\")\n",
    "final_acc = evaluate_model(model, val_data, batch_size=16)\n",
    "print(\"Final validation accuracy:\", final_acc)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
